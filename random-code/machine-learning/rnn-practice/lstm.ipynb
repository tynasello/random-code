{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-CL4iDCZeI8"
      },
      "source": [
        "## Sentiment Analysis for Amazon Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGM3tgKbONIZ"
      },
      "source": [
        "The goal is to predict if a Amazon review is of positive or negative sentiment.\n",
        "\n",
        "Following this blog post https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kttX3aAOhSDX",
        "outputId": "621a5978-a524-4fa8-8d68-5c6a299c8530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX4DSdNBADCr",
        "outputId": "61a706d1-c022-4ada-fe63-bbb383b7c9f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import re \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvMg453SyQTY"
      },
      "source": [
        "Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KB5lp3vnyRWp"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    \n",
        "    \n",
        "def to_t(tensor):\n",
        "    #convert tensor to gpu if available\n",
        "    return tensor.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq6wr-r7c7MI"
      },
      "source": [
        "Load and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "Hjk9fbZdhsno"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/RNNs/lstm/train.ft.txt') as f:\n",
        "    train_file = f.readlines()\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/RNNs/lstm/test.ft.txt') as f:\n",
        "    test_file = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "58vdcbYueIxI"
      },
      "outputs": [],
      "source": [
        "# data set gives 3,600,000 training samples and 400,000 test samples\n",
        "# we are going to use 300,000 training samples and 50,000 test samples to speed up training\n",
        "num_train_samples = int(3e5)\n",
        "num_test_samples = int(5e4)\n",
        "\n",
        "train_sentences = [sentence.split(' ', 1)[1][:-1].lower() for sentence in train_file[:num_train_samples]]\n",
        "test_sentences = [sentence.split(' ', 1)[1][:-1].lower() for sentence in test_file[:num_test_samples]]\n",
        "\n",
        "# 0 if negative sentiment (__label__1), 1 if positive (__label__2)\n",
        "\n",
        "train_y = np.array([0 if sentence.split(' ')[0] == '__label__1' else 1 for sentence in train_file])\n",
        "test_y = np.array([0 if sentence.split(' ')[0] == '__label__1' else 1 for sentence in test_file])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "gyIfXfaqg3Sk"
      },
      "outputs": [],
      "source": [
        "train_x = []\n",
        "test_x = []\n",
        "\n",
        "vocabulary = Counter()\n",
        "\n",
        "# tokenize each sentence and add tokens to vocabulary collection.\n",
        "for i, sentence in enumerate(train_sentences):\n",
        "    tokenized_sentence = []\n",
        "\n",
        "    for word in word_tokenize(sentence):\n",
        "        word = word.lower()\n",
        "        # update vocabulary dict with word\n",
        "        vocabulary.update([word])\n",
        "        tokenized_sentence.append(word)\n",
        "\n",
        "    train_x.append(tokenized_sentence)\n",
        "\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "    tokenized_sentence = []\n",
        "\n",
        "    for word in word_tokenize(sentence):\n",
        "        word = word.lower()\n",
        "        tokenized_sentence.append(word)\n",
        "\n",
        "    test_x.append(tokenized_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "ekvUjtnpuuCB"
      },
      "outputs": [],
      "source": [
        "# mappings for tokens and ids in vocabulary\n",
        "word_to_id = {word:i for i,word in enumerate(Counter(['PADDING_CONST']) + vocabulary)}\n",
        "id_to_word = {i:word for i,word in enumerate(Counter(['PADDING_CONST']) + vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "KXm_2UpMnrCW"
      },
      "outputs": [],
      "source": [
        "# convert each sentence to an array of ids corresponding to each token\n",
        "for x in (train_x, test_x):\n",
        "    for i, sentence in enumerate(x):\n",
        "        x[i] = [word_to_id[token] if token in word_to_id else 0 for token in sentence]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "JOaup-xNrdw2"
      },
      "outputs": [],
      "source": [
        "# make all sentences in data set of the same length\n",
        "# pad shorter sentences with 0s and cut longer sentences\n",
        "\n",
        "max_sentence_length = 150\n",
        "\n",
        "adjusted_x = np.zeros((len(train_x), max_sentence_length), dtype=int)\n",
        "for i, sentence in enumerate(train_x):\n",
        "    adjusted_x[i][:min(max_sentence_length, len(sentence))] = sentence[:min(max_sentence_length, len(sentence))]\n",
        "train_x = adjusted_x\n",
        "\n",
        "adjusted_x = np.zeros((len(test_x), max_sentence_length), dtype=int)\n",
        "for i, sentence in enumerate(test_x):\n",
        "    adjusted_x[i][:min(max_sentence_length, len(sentence))] = sentence[:min(max_sentence_length, len(sentence))]\n",
        "test_x = adjusted_x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prmhEtNz4uIp",
        "outputId": "194c314c-7810-4d3d-f40e-a3afcf3bbe88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 13,  2, 24, 25, 26, 27, 28, 29, 30, 31,\n",
              "       12, 21, 32, 33,  4, 30, 34, 35, 36, 37, 38, 39, 38,  4, 40, 21, 32,\n",
              "       41, 33, 13, 42,  4, 43, 31, 12, 13, 44, 45, 46, 47, 48, 49, 50, 51,\n",
              "       52, 53, 54, 55, 56, 49, 57, 58, 29, 13, 22, 59, 60, 26, 61, 24, 62,\n",
              "       12, 63,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
            ]
          },
          "execution_count": 186,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_x[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "W5IqFim-7R4e"
      },
      "outputs": [],
      "source": [
        "# convert data np arrays to pytorch tensors\n",
        "train_x = to_t(torch.from_numpy(train_x))\n",
        "test_x = to_t(torch.from_numpy(test_x))\n",
        "\n",
        "train_y = to_t(torch.from_numpy(train_y))\n",
        "test_y = to_t(torch.from_numpy(test_y))\n",
        "\n",
        "# data sets\n",
        "\n",
        "class AmazonSentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenized_sentences, sentiments):\n",
        "        self.tokenized_sentences = tokenized_sentences\n",
        "        self.sentiments = sentiments\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_sentences)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        item = (\n",
        "            self.tokenized_sentences[i],\n",
        "            self.sentiments[i]\n",
        "        )\n",
        "        return item \n",
        "        \n",
        "train_dataset = AmazonSentimentDataset(train_x, train_y)\n",
        "test_dataset = AmazonSentimentDataset(test_x, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "E6tgLmriBBNK"
      },
      "outputs": [],
      "source": [
        "# data loaders\n",
        "\n",
        "batch_size = 250\n",
        "\n",
        "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "data_loaders = {'train': train_dl, 'test': test_dl}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRxaxKeJFWJ-"
      },
      "source": [
        "### Amazon Review Sentiment Analysis Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "L0HIbC3wA210"
      },
      "outputs": [],
      "source": [
        "# model architecture from https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/\n",
        "\n",
        "class ARSAModel(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, num_layers, num_embeddings, embedding_dim):\n",
        "        super(ARSAModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        # lookup table for word embeddings\n",
        "        self.embedding = to_t(torch.nn.Embedding(num_embeddings, embedding_dim))\n",
        "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, h_0, c_0):\n",
        "        # h_0 -> initial hidden state\n",
        "        # c_0 -> initial cell state\n",
        "\n",
        "        # get word embeddings for vectors in inputted batch\n",
        "        # word embeddigns are jointly learned as the model is trained\n",
        "        embeddings = self.embedding(x)\n",
        "        out, hidden = self.lstm(embeddings, (h_0, c_0))\n",
        "\n",
        "        # reshape out for fully connected layer\n",
        "        out = out.reshape(-1, self.hidden_size)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "\n",
        "        # reshape output to represent probabilities of positive sentiment for each input in batch\n",
        "        out = out.view(batch_size, -1)\n",
        "        out = out[:,-1]\n",
        "\n",
        "        return out, h_0, c_0\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # return tuple containing two tensors of zeroes - (initial hidden state, initial cell state)\n",
        "        # change next model parameters to zeroes in the process\n",
        "        weight = next(self.parameters())\n",
        "        \n",
        "        return (\n",
        "            weight.new(self.num_layers, batch_size, self.hidden_size).zero_().to(device),\n",
        "            weight.new(self.num_layers, batch_size, self.hidden_size).zero_().to(device)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "UEM18WT1LRmw"
      },
      "outputs": [],
      "source": [
        "model = ARSAModel(\n",
        "    hidden_size = 512,\n",
        "    output_size = 2,\n",
        "    num_layers = 2,\n",
        "    num_embeddings = len(word_to_id),\n",
        "    embedding_dim = max_sentence_length,\n",
        ")\n",
        "\n",
        "# model to gpu if available\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOsjNEcgVvr0"
      },
      "source": [
        "Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV7yLbqWIAbA",
        "outputId": "e9773116-9fe7-487a-d863-cc36263a51b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0, Loss: 0.6941\n",
            "Step 250, Loss: 0.4897\n",
            "Step 500, Loss: 0.2834\n",
            "Step 750, Loss: 0.1919\n",
            "Step 1000, Loss: 0.2378\n",
            "Epoch [1/1], Loss: 0.2425\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    h_0, c_0 = model.init_hidden()\n",
        "\n",
        "    for i, (x, y) in enumerate(data_loaders['train']):\n",
        "        # x and y are batches\n",
        "\n",
        "        # don't accumlate gradients        \n",
        "        optimizer.zero_grad()\n",
        "        model.zero_grad()\n",
        "\n",
        "        h_0 = h_0.detach()\n",
        "        c_0 = c_0.detach()\n",
        "\n",
        "        output, h_0, c_0 = model(x, h_0, c_0)\n",
        "\n",
        "        # calculate loss using Binary Cross Entropy loss function\n",
        "        loss = criterion(output, y.float())\n",
        "        # calculate model gradients\n",
        "        loss.backward()\n",
        "        # step model parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        if(i%250==0):\n",
        "            print(\n",
        "                f\"Step {i}, \"\n",
        "                f\"Loss: {loss.item():.4f}\"\n",
        "            )\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
        "        f\"Loss: {loss.item():.4f}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz-UBmEW8qIl"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxBm4NIlohn9"
      },
      "source": [
        "Testing model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrT47PVThoN2",
        "outputId": "b1b3e1a6-84c7-43cd-aca6-d4c91d028377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 90.6220%\n"
          ]
        }
      ],
      "source": [
        "num_correct = 0\n",
        "\n",
        "h_0, c_0 = model.init_hidden()\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for x, y in data_loaders['test']:\n",
        "        h_0 = h_0.detach()\n",
        "        c_0 = c_0.detach()\n",
        "        output, h_0, c_0 = model(x, h_0, c_0)\n",
        "        \n",
        "        pred = torch.round(output)\n",
        "        correct = pred.eq(y.float())\n",
        "\n",
        "        correct = correct.cpu().numpy()\n",
        "        num_correct += np.sum(correct)\n",
        "\n",
        "print(f\"Accuracy: {num_correct / (len(data_loaders['test']) * batch_size) * 100:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6nONQ9z6J61"
      },
      "source": [
        "To improve model accuracy, we could train on the complete training dataset, alter the model architecture, tune hyperparameters, use pretrained word embeddings, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-ii9buY82FV"
      },
      "source": [
        "Running personal test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg8qCTO8rFNA",
        "outputId": "84c578a3-51ca-40db-89ea-fd90a91b390f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: I recently bought a product from a different brand that was terrible. This product was similar. \n",
            "Predicted Sentiment: Negative \n",
            "\n",
            "Sentence: Someone got this for me as a gift. I didnt like it at first but now I need more. Its wonderful. \n",
            "Predicted Sentiment: Positive \n",
            "\n",
            "Sentence: I wish I had something good to say about this product, but I dont. \n",
            "Predicted Sentiment: Negative \n",
            "\n"
          ]
        }
      ],
      "source": [
        "h_0, c_0 =  (\n",
        "    to_t(torch.zeros(model.num_layers, 250, model.hidden_size)),\n",
        "    to_t(torch.zeros(model.num_layers, 250, model.hidden_size))\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "\n",
        "    for sentence in [\n",
        "        'I recently bought a product from a different brand that was terrible. This product was similar.',\n",
        "        'Someone got this for me as a gift. I didnt like it at first but now I need more. Its wonderful.',\n",
        "        'I wish I had something good to say about this product, but I dont.'\n",
        "    ]:\n",
        "    \n",
        "        tokenized_sentence = []\n",
        "\n",
        "        for word in word_tokenize(sentence):\n",
        "            word = word.lower()\n",
        "            tokenized_sentence.append(word)\n",
        "\n",
        "        tokenized_sentence = [word_to_id[token] if token in word_to_id else 0 for token in tokenized_sentence]\n",
        "        adjusted_x = np.zeros((1, max_sentence_length), dtype=int)\n",
        "        adjusted_x[0][:min(max_sentence_length, len(tokenized_sentence))] = tokenized_sentence[:min(max_sentence_length, len(tokenized_sentence))]\n",
        "        x = to_t(torch.from_numpy(adjusted_x)).repeat(batch_size, 1)\n",
        "\n",
        "        output, _, _ = model(x, h_0, c_0)\n",
        "\n",
        "        pred = torch.round(output)\n",
        "        print(\n",
        "            f\"Sentence: {sentence} \\n\"\n",
        "            f\"Predicted Sentiment: {'Positive' if pred[0].item() == 1.0 else 'Negative'} \\n\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzZu0S9C9EOy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lstm.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
